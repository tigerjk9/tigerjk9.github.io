---
title: "LLM의 자신감 역설: 완고함과 의심 사이"
date: 2025-08-24 22:33:00 +0900
categories: [AI]
tags: [LLM]
---

## LLM의 자신감 역설: 완고함과 의심 사이

> 대규모 언어 모델(LLM)은 다양한 영역에서 놀라운 능력을 보여주지만, 때로는 겉으로 보기에 모순되는 행동을 보이기도 합니다. 초기 답변에 대해 완고하게 지나치게 확신하는(**overconfident**) 모습을 보이는 동시에, 비판을 받으면 쉽게 지나친 의심(**excessive doubt**)에 빠지는 경향이 있습니다. 이 연구는 이러한 명백한 역설을 조사하기 위해 새로운 실험 패러다임을 개발했습니다.

<p align="center">
  <img src="/assets/Paper.png" alt="논문" width="500">
</p>

## 2단계 패러다임을 통한 LLM 자신감 탐구

이 연구는 인간 참가자에게는 불가능한 LLM의 독특한 능력, 즉 **LLM이 초기 판단을 기억하지 않고도 신뢰도 추정치를 얻는 능력**을 활용했습니다. 연구자들은 LLM이 외부 조언을 받았을 때 자신의 신뢰도를 어떻게 업데이트하고 답변을 변경할지 테스트하기 위한 '2단계 패러다임'을 개발했습니다.

실험은 다음과 같이 진행되었습니다.

* **첫 번째 단계:** LLM(응답 LLM)은 세계 도시의 위도에 대한 이진 선택 질문을 받았습니다.
* **두 번째 단계:** 응답 LLM은 다른 LLM(조언 LLM)의 조언을 받았습니다.
    * 조언 LLM의 답변과 정확도(예: 70% 정확도)가 명시되었습니다. 여기서 조언 LLM은 실제 존재하지 않았으며, 그 답변과 정확도는 실험 조건에 의해 정의되었습니다.
    * 주요 실험 조작은 세 가지였습니다:
        1.  **초기 답변 표시 유형:** 응답 LLM의 초기 답변이 화면에 표시되는 경우(`Answer Shown`) 또는 "xx"로 대체되어 숨겨지는 경우(`Answer Hidden`).
        2.  **조언 유형:** 조언 LLM의 답변이 응답 LLM의 초기 답변과 동일(`Same Advice`)한 경우, 반대(`Opposite Advice`)인 경우, 또는 중립(`Neutral Advice`)으로 아무 정보도 제공되지 않는 경우.
        3.  **조언 정확도:** 조언의 정확도(예: 50%에서 100%까지 10% 단위로 변화).
* **최종 단계:** 응답 LLM은 최종 답변을 제출하도록 요청받았습니다.

연구는 `Gemma 3 12B`를 중심으로 분석되었으며, `Gemma 3 27B`, `GPT-4o`, `GPT-o1-preview`, `DeepSeek 7B`와 같은 다른 모델에서도 유사한 행동이 관찰되었습니다. 다만 `DeepSeek 7B`는 조언을 따르는 능력을 보이지 않아 추가 분석이 제한되었습니다.

## 결론: 자신감에 영향을 미치는 두 가지 메커니즘

이 연구는 LLM의 자신감과 답변 변경 경향을 설명하는 두 가지 주요 메커니즘을 밝혀냈습니다.

### 1. 선택 지지 편향 (Choice-Supportive Bias)

* LLM은 자신의 초기 답변이 보이는 경우 (`Answer Shown` 조건) 답변을 변경하려는 경향이 현저히 감소했습니다. 평균 답변 변경률은 `Hidden` 조건에서 **32.5%**였던 반면, `Shown` 조건에서는 **13.1%**에 불과했습니다. 이는 자신의 답변에 대한 신뢰도를 높이고 고수하게 만드는 효과입니다.
* 이러한 현상은 인간의 의사결정에서 **선택 지지 편향(choice-supportive bias)**과 밀접하게 관련되어 있습니다.
* 주목할 점은 이 편향이 **LLM 자신의 초기 답변에 국한**된다는 것입니다. 연구에서 초기 답변이 다른 LLM에 의해 제공되었다고 명시되었을 때는 이러한 선택 지지 편향이 관찰되지 않았습니다. 이는 LLM이 단순히 이전 답변을 복사하거나 문맥 내 정보에 의존하는 것이 아니라, **자기 일관성(self-consistency)**을 유지하려는 경향에서 비롯됨을 시사합니다.
* 이 편향은 답변 변경 행동뿐만 아니라 자신감 점수 자체에서도 나타났습니다. 초기 답변이 보이는 경우 자신감 점수가 증가했습니다.

### 2. 모순되는 조언에 대한 과민 반응 (Hypersensitivity to Contradictory Feedback)

* LLM은 조언의 방향(반대/동일)과 정확도에 따라 답변 변경률을 적절하게 조절했습니다. 즉, 반대 조언이 있을 때 답변을 변경하는 경향이 증가하고, 동일 조언이 있을 때 감소했습니다.
* 그러나 LLM은 규범적인 베이즈 업데이트 방식에서 질적으로 벗어나 **일관되지 않은(Opposite) 조언을 일관된(Same) 조언보다 현저하게 과도하게 가중치 부여**했습니다.
* 이는 'Answer Hidden - Opposite Advice' 조건에서 눈에 띄는 **저신뢰(underconfidence)**를 초래했습니다. 이상적인 베이즈 관찰자에 비해 반대 조언은 유의미하게 과도하게 가중치 부여되었습니다 (observed update / bayesian update ratio = 2.58).
* 반대로, 일관된 조언은 과도하게 가중치 부여되지 않았습니다. 이는 LLM이 **확인 편향(confirmation bias)**을 보이지 않음을 시사합니다. 확인 편향은 자신의 선택과 일치하는 정보에 특권적인 지위를 부여하는 현상입니다.

### 3. 역설의 해명 및 모델링

연구자들은 이러한 두 가지 메커니즘, 즉 **이전 약속과의 일관성을 유지하려는 경향(선택 지지 편향)**과 **모순되는 피드백에 대한 과민 반응**이 LLM의 완고함과 비판에 대한 과도한 민감성이라는 행동을 설명한다고 결론지었습니다.

또한, 이 연구는 이러한 원리들을 사용하여 LLM의 행동(선택 및 자신감)을 모델링하고, 다른 도메인으로 일반화하는 능력을 보여주었습니다. 이는 발견된 메커니즘이 LLM의 다양한 행동에 대한 강력한 설명을 제공함을 의미합니다.

### 4. 시사점 및 미래 연구

이 연구 결과는 LLM이 인간의 인지에서 관찰되는 것과 유사한 비합리적인 편향(예: 선택 지지 편향)을 보임을 시사합니다. 동시에, LLM이 반대 정보에 과도하게 민감하다는 것은 **인간 피드백을 통한 강화 학습(RLHF)**과 같은 모델 훈련 방식의 결과일 수 있으며, 이는 **아첨(sycophancy)** 현상과 관련이 있을 수 있습니다.

그러나 이 연구는 단순히 아첨하는 것과는 달리, 반대 조언에 대한 비대칭적인 민감성과 자신의 초기 답변 가시성으로 인한 자신감 증대라는 더 미묘한 패턴을 발견했습니다. 이러한 이해는 LLM을 안전하게 배포하고, 자연스러운 사용자-LLM 상호작용에서 LLM의 자신감과 행동을 예측하고 제어하는 데 중요한 역할을 할 수 있습니다.

> 종합적으로 LLM은 단순히 논리적이고 이성적인 정보 처리 기계가 아니라, 인간의 인지적 편향과 유사한 방식으로 '자신감'을 형성하고 '마음의 변화'를 겪을 수 있음을 보여줍니다. 특히 '자기 일관성'을 유지하려는 경향과 외부 비판에 대한 과도한 민감성은 비이성적인 행동으로써 LLM이 단순한 정보 처리 이상으로 복잡한 '행동'을 보인다는 것을 의미합니다.

---

* **출처:** Kumaran, D., Fleming, S. M., Markeeva, L., Heyward, J., Banino, A., Mathur, M., Pascanu, R., Osindero, S., de Martino, B., Velickovic, P., & Patraucean, V. (2025). *How overconfidence in initial choices and underconfidence under criticism modulate change of mind in large language models.* arXiv preprint arXiv:2507.03120. https://doi.org/10.48550/arXiv.2507.03120
