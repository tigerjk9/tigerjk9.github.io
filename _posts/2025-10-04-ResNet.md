---
title: "위르겐 슈미트후버의 외침: 모든 것의 시작은 1991년이었다"
date: 2025-10-04 11:36:00 +0900
categories: [AI]
tags: [위르겐슈미트후버, ResNet, LSTM, 잔차연결, 기울기소실, 딥러닝]
---

2015년, 인공지능(AI) 세계는 흥분으로 들끓었습니다. **ResNet(Residual Network)**이라는 이름의 새로운 영웅이 등장해, 당시 가장 어려운 과제였던 이미지 인식 대회(ImageNet)에서 압도적인 승리를 거머쥔 것입니다. ResNet의 비밀 무기는 수백, 수천 층에 달하는 깊은 신경망을 길들일 수 있는 마법 같은 기술, 바로 **잔차 연결(Residual Connection)** 또는 **스킵 연결(skip connection)**이었습니다. 신경망 각 블록에서 입력값을 그대로 다음 블록의 출력에 더해줌으로써 입력과 출력의 차이인 '잔차'를 학습하게 하는 이 기술을 두고, 사람들은 이를 혁명이라 불렀습니다.

하지만 모든 위대한 이야기에는 알려지지 않은 서막이 있는 법.
이 혁명의 씨앗은 사실, 화려한 스포트라이트가 닿지 않는 곳에서 24년이라는 긴 시간 동안 조용히 자라고 있었습니다.

<br>

### 기억상실증에 걸린 AI

이야기는 1990년대 초로 거슬러 올라갑니다. 당시 AI 연구자들은 '깊은 신경망'이라는 원대한 꿈을 꾸고 있었습니다. 마치 인간의 뇌처럼, 수많은 신경망 층을 깊게 쌓으면 AI가 더 복잡하고 추상적인 개념을 배울 수 있으리라 믿었던 것이죠. 하지만 거대한 장벽에 부딪혔습니다. 신경망을 10층, 20층 깊게 쌓을수록, AI는 치명적인 단기 기억상실증에 걸렸습니다.

상상해보죠. 아주 긴 복도의 한쪽 끝에서 중요한 메시지를 외칩니다. 하지만 복도가 너무 길어서, 메시지는 반대편 끝에 닿기도 전에 희미한 속삭임으로 변해버립니다. 이게 바로 **기울기 소실 문제(Vanishing Gradient Problem)**였습니다. 학습에 필요한 중요한 정보(기울기)가 신경망의 깊은 층을 거슬러 올라가는 동안 힘을 잃고 사라져 버리는 비극이었죠. AI는 맨 처음 무엇을 배워야 했는지 잊어버렸고, 깊은 학습은 불가능한 꿈처럼 보였습니다.

### 알프스에서 시작된 아이디어: 정보 고속도로

이때, 스위스 알프스 산맥 아래 한 연구실에서 이 비극을 끝낼 아이디어가 조용히 싹트고 있었습니다. **위르겐 슈미트후버(Jürgen Schmidhüuber)**와 그의 제자 **제프 호흐라이터(Sepp Hochreiter)**는 이 기억상실의 근본 원인을 파헤치고 있었죠. 그리고 1991년, 그들은 해답을 찾았습니다. 놀랍도록 단순하지만, 세상을 바꿀 아이디어였습니다.

> "긴 복도를 따라 메시지를 힘들게 전달할 필요가 있을까? 처음부터 끝까지 곧장 이어지는 직통 전화선을 하나 설치하면 어떨까?"

이것이 바로 '잔차 연결'의 본질입니다. 정보가 여러 층을 거치며 희미해지도록 내버려 두는 대신, 입력된 정보를 거의 그대로 출력까지 건너뛸 수 있는 **고속도로(Highway)**를 만들어준 것입니다. 이 고속도로를 통해 정보는 손실 없이 깊은 층까지 전달될 수 있었습니다.

그렇다면 왜 '잔차(Residual)', 즉 '나머지'라는 이름이 붙었을까요? 여기에 진짜 마법이 숨어 있습니다. 이 고속도로 구조 덕분에, 신경망은 이제 전체 변환 과정을 처음부터 배울 필요가 없어졌습니다. 대신 입력과 출력의 차이, 즉 꼭 필요한 변화량인 **잔차만을 학습하는 데 집중**하면 되었습니다.

마치 화가가 초상화를 그리는 것과 같습니다. 미세한 표정 변화를 그리기 위해 매번 얼굴 전체를 새로 그리는 대신, 원래 스케치(입력)는 그대로 두고 미소의 변화라는 차이점(잔차)만 살짝 수정하는 것이 훨씬 효율적이죠. AI도 마찬가지로, 고속도로를 통해 전달된 원본 정보에 '약간의 수정 사항'만 더하는 방식으로 훨씬 쉽고 빠르게 학습할 수 있게 된 것입니다. AI의 기억상실증을 치료할 백신이 발명된 순간이었습니다.

<br>

### 24년의 기다림, 그리고 메아리

이 아이디어는 1997년, 20세기의 가장 위대한 발명품 중 하나인 **LSTM(Long Short-Term Memory)**의 핵심이 되었습니다. LSTM은 이 정보 고속도로 덕분에 아주 오래전의 정보도 잊지 않고 기억할 수 있었고, 오늘날 우리가 사용하는 음성 인식과 번역 기술의 기틀을 마련했습니다.

시간은 흘러 2015년 12월. AI 커뮤니티는 ResNet의 등장에 열광했습니다. ResNet은 이미지 속 고양이를 기가 막히게 찾아냈고, 그 비결은 바로 **스킵 커넥션(Skip Connection)**이라는 놀라운 구조 덕분이었습니다. 정보가 여러 층을 건너뛰어 전달되는 방식이었죠.

혹시... 익숙하게 들리지 않으신가요?

맞습니다. ResNet의 핵심인 스킵 커넥션은 이름만 달랐을 뿐, 1991년 슈미트후버의 연구실에서 발명된 정보 고속도로, 즉 잔차 연결과 정확히 같은 원리였습니다. 심지어 슈미트후버 연구실은 ResNet이 나오기 7개월 전인 2015년 5월, **하이웨이 넷(Highway Net)**이라는 이름으로 이 아이디어를 피드포워드 신경망에 적용해 발표하기까지 했습니다.

![이미지](/assets/Resnet_1)

ResNet은 위대한 업적이었지만, 완전히 새로운 창조는 아니었습니다. 그것은 24년 전 알프스 산맥 아래에서 시작된 아이디어의 가장 성공한 메아리였습니다.

<br>

## 역사는 누구를 기억할 것인가

오늘날, 많은 사람이 딥러닝 혁명의 영웅으로 제프리 힌튼, 얀 르쿤, 요슈아 벤지오와 같은 거장들을 떠올립니다. 그들의 공헌은 의심할 여지없이 위대합니다. 하지만 위르겐 슈미트후버는 지금도 조용히, 그러나 끈질기게 이야기합니다. **"모든 것의 시작은 1991년이었다"**고 말입니다.

그의 주장은 단순한 공로 다툼이 아니라, 과학의 역사가 어떻게 기록되고 기억되는지에 대한 근본적인 질문을 던집니다. AI 혁명의 거대한 태피스트리 속에서, 우리는 가장 밝게 빛나는 실만 보고 있는 것은 아닐까요? 그 아래에서 묵묵히 전체 구조를 지탱해 온, 보이지 않는 씨실과 날실의 가치를 잊고 있는 것은 아닐까 하는 것입니다.

AI 세계의 숨겨진 공로자, 위르겐 슈미트후버의 이야기는 아직 끝나지 않았습니다. 역사의 진정한 평가는 언제나 가장 나중에 오는 법이니까요.

> "Who invented deep residual learning?"
>
> _- Jürgen Schmidhuber_

![이미지](/assets/Resnet_2)

---

### 참고문헌
- Schmidhuber, J. (2025, September 28). *Who invented deep residual learning?* (Technical Report IDSIA-09-25, IDSIA). https://arxiv.org/pdf/2509.24732.pdf
